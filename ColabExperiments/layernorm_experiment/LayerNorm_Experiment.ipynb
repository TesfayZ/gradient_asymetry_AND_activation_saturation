{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization Experiment - Testing if LayerNorm Prevents Saturation\n",
    "\n",
    "**Hypothesis:** LayerNorm normalizes pre-activations to ~0 mean, ~1 std before tanh, keeping outputs in the linear region where gradients flow properly.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "| Network | Hidden Activations | Pre-Output | Output Activation | Architecture |\n",
    "|---------|-------------------|------------|-------------------|---------------|\n",
    "| Actor   | ReLU | **LayerNorm** | tanh | 7->64->32->LN->tanh->3 |\n",
    "| Critic  | ReLU | None | linear | 510->512->128->1 |\n",
    "\n",
    "## Learning Rate Order (High to Low)\n",
    "Experiments run from **HIGH to LOW** learning rates to prioritize early-stopping cases:\n",
    "- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]`\n",
    "- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]`\n",
    "- Total: **16 experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Setup - Choose Start Mode\n\nThe experiment script **automatically restores from Google Drive** when you run it. Choose how to proceed:\n\n- **Option 1: Continue** - Run experiments (auto-restores completed ones from Drive)\n- **Option 2: Start Fresh** - Clear ALL results (local + Drive) and start from scratch  \n- **Option 3: Import from Zip** - Import results from a downloaded zip file"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport zipfile\nimport shutil\nimport json\n\nEXPERIMENT_NAME = 'layernorm_experiment'\nEXPERIMENT_ZIP = f'{EXPERIMENT_NAME}.zip'\nRESULTS_DIR = f'/content/results/{EXPERIMENT_NAME}'\nEXPERIMENT_DIR = f'/content/{EXPERIMENT_NAME}'\nDRIVE_BACKUP_DIR = '/content/drive/MyDrive/layernorm_results'\n\n# ============================================================\n# STEP 1: Extract experiment files\n# ============================================================\nif not os.path.exists(f'{EXPERIMENT_DIR}/mec_env.py'):\n    drive_zip = f'/content/drive/MyDrive/{EXPERIMENT_ZIP}'\n    \n    if os.path.exists(drive_zip):\n        print(f\"Found experiment zip in Drive: {drive_zip}\")\n        zip_path = drive_zip\n    else:\n        print(f\"Upload {EXPERIMENT_ZIP}:\")\n        from google.colab import files\n        uploaded = files.upload()\n        zip_path = list(uploaded.keys())[0]\n    \n    print(f\"Extracting {zip_path}...\")\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        z.extractall('/content')\n    print(\"Experiment files extracted!\")\nelse:\n    print(\"Experiment files already present!\")\n\n# ============================================================\n# STEP 2: Check existing progress\n# ============================================================\ndrive_status_file = os.path.join(DRIVE_BACKUP_DIR, 'experiment_status.json')\nif os.path.exists(drive_status_file):\n    with open(drive_status_file) as f:\n        drive_status = json.load(f)\n    drive_completed = len(drive_status.get('completed', []))\n    in_progress = drive_status.get('in_progress')\n    print(f\"\\n*** Found Drive backup: {drive_completed}/16 completed ***\")\n    if in_progress:\n        print(f\"    Last in progress: {in_progress} (will restart this one)\")\nelse:\n    drive_completed = 0\n    print(\"\\n*** No existing Drive backup found ***\")\n\n# ============================================================\n# CHOOSE START MODE\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SELECT START MODE\")\nprint(\"=\" * 60)\nprint(\"1. Continue - Auto-restore from Drive and run remaining\")\nprint(\"2. Start Fresh - Clear ALL results and start from scratch\")\nprint(\"3. Import from Zip - Upload results zip to restore\")\nprint(\"=\" * 60)\n\nstart_mode = input(\"Enter choice (1/2/3): \").strip()\n\nif start_mode == '2':\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STARTING FRESH - Clearing all results\")\n    print(\"=\" * 60)\n    if os.path.exists(RESULTS_DIR):\n        shutil.rmtree(RESULTS_DIR)\n        print(f\"Cleared: {RESULTS_DIR}\")\n    if os.path.exists(DRIVE_BACKUP_DIR):\n        shutil.rmtree(DRIVE_BACKUP_DIR)\n        print(f\"Cleared: {DRIVE_BACKUP_DIR}\")\n    print(\"Ready to run all 16 experiments from scratch!\")\n\nelif start_mode == '3':\n    print(\"\\n\" + \"=\" * 60)\n    print(\"IMPORT FROM ZIP\")\n    print(\"=\" * 60)\n    \n    print(\"Upload your results zip file:\")\n    from google.colab import files\n    uploaded = files.upload()\n    results_zip_path = list(uploaded.keys())[0]\n    \n    temp_dir = '/content/temp_import'\n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir)\n    os.makedirs(temp_dir)\n    \n    with zipfile.ZipFile(results_zip_path, 'r') as z:\n        z.extractall(temp_dir)\n    \n    # Find experiment_status.json\n    actual_results = temp_dir\n    for root, dirs, files_list in os.walk(temp_dir):\n        if 'experiment_status.json' in files_list:\n            actual_results = root\n            break\n    \n    # Copy to Drive backup directory (so auto-restore picks it up)\n    if os.path.exists(DRIVE_BACKUP_DIR):\n        shutil.rmtree(DRIVE_BACKUP_DIR)\n    shutil.copytree(actual_results, DRIVE_BACKUP_DIR)\n    \n    with open(os.path.join(DRIVE_BACKUP_DIR, 'experiment_status.json')) as f:\n        imported_status = json.load(f)\n    print(f\"Imported {len(imported_status.get('completed', []))} completed experiments to Drive\")\n    print(\"The experiment script will auto-restore these when you run it.\")\n    \n    shutil.rmtree(temp_dir)\n\nelse:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONTINUE MODE\")\n    print(\"=\" * 60)\n    if drive_completed > 0:\n        print(f\"Will auto-restore {drive_completed} completed experiments from Drive\")\n    print(\"Ready to run!\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SETUP COMPLETE - Run the next cell to start experiments\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/content')\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify LayerNorm Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/layernorm_experiment')\n",
    "\n",
    "from Model import LayerNormActorNetwork, CriticNetwork, ActorNetwork\n",
    "import torch\n",
    "\n",
    "# Compare architectures\n",
    "original_actor = ActorNetwork(7, 3, torch.tanh)\n",
    "layernorm_actor = LayerNormActorNetwork(7, 3, torch.tanh)\n",
    "critic = CriticNetwork(350, 150, 7, 3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal Actor: 7 -> 64 -> 32 -> tanh -> 3\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in original_actor.parameters()):,}\")\n",
    "print(f\"\\nLayerNorm Actor: 7 -> 64 -> 32 -> LayerNorm -> tanh -> 3\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in layernorm_actor.parameters()):,}\")\n",
    "print(f\"\\nCritic: 510 -> 512 -> 128 -> 1 (linear)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
    "\n",
    "# Show the LayerNorm layer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LayerNorm Actor Modules:\")\n",
    "print(\"=\" * 60)\n",
    "for name, module in layernorm_actor.named_modules():\n",
    "    if name:\n",
    "        print(f\"  {name}: {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run All 16 Experiments\n",
    "\n",
    "This will run all 16 experiments with **HIGH learning rates FIRST**:\n",
    "- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "\n",
    "**Progress is auto-saved to Google Drive every 100 episodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "sys.path.insert(0, '/content/layernorm_experiment')\n",
    "\n",
    "from run_layernorm_experiment import run_all_experiments\n",
    "\n",
    "# Run all experiments\n",
    "run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Check Experiment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "status_file = os.path.join(results_dir, 'experiment_status.json')\n",
    "\n",
    "if os.path.exists(status_file):\n",
    "    with open(status_file) as f:\n",
    "        status = json.load(f)\n",
    "    print(\"Experiment Status:\")\n",
    "    print(f\"  Completed: {len(status['completed'])}/16\")\n",
    "    if status['in_progress']:\n",
    "        print(f\"  In progress: {status['in_progress']}\")\n",
    "    print(\"\\nCompleted experiments:\")\n",
    "    for exp in sorted(status['completed']):\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No status file found yet.\")\n",
    "\n",
    "# Also check Drive backup\n",
    "drive_dir = '/content/drive/MyDrive/layernorm_results'\n",
    "if os.path.exists(drive_dir):\n",
    "    print(f\"\\nDrive backup exists: {drive_dir}\")\n",
    "    contents = os.listdir(drive_dir)\n",
    "    print(f\"  Contents: {contents[:5]}...\" if len(contents) > 5 else f\"  Contents: {contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"LayerNorm Experiment Results Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Actor LR':<12} {'Critic LR':<12} {'Stop Ep.':<12} {'Final Reward':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for exp_dir in sorted(os.listdir(results_dir)):\n",
    "        result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "        if os.path.exists(result_file):\n",
    "            with open(result_file) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "    \n",
    "    for data in sorted(results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        print(f\"{data['actor_lr']:<12} {data['critic_lr']:<12} {data['stopping_episode']:<12} {data['final_reward']:<15.4f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No results directory found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Compare with Original (No LayerNorm) Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_results(results_dir):\n",
    "    results = []\n",
    "    if os.path.exists(results_dir):\n",
    "        for exp_dir in os.listdir(results_dir):\n",
    "            result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file) as f:\n",
    "                    results.append(json.load(f))\n",
    "    return results\n",
    "\n",
    "layernorm_results = load_results('/content/results/layernorm_experiment')\n",
    "original_results = load_results('/content/results/stopping_experiment')\n",
    "\n",
    "# Also try Drive backups\n",
    "if not original_results:\n",
    "    original_results = load_results('/content/drive/MyDrive/gradient_asymmetry_results')\n",
    "\n",
    "if original_results and layernorm_results:\n",
    "    print(\"COMPARISON: Original (No LayerNorm) vs LayerNorm Actor\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"{'Actor LR':<10} {'Critic LR':<10} {'Original':<15} {'LayerNorm':<15} {'Difference':<15}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    for orig in sorted(original_results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        ln = next((r for r in layernorm_results \n",
    "                   if r['actor_lr'] == orig['actor_lr'] and r['critic_lr'] == orig['critic_lr']), None)\n",
    "        if ln:\n",
    "            diff = ln['stopping_episode'] - orig['stopping_episode']\n",
    "            diff_str = f\"+{diff}\" if diff > 0 else str(diff)\n",
    "            improved = \"IMPROVED\" if diff > 100 else \"\"\n",
    "            print(f\"{orig['actor_lr']:<10} {orig['critic_lr']:<10} {orig['stopping_episode']:<15} {ln['stopping_episode']:<15} {diff_str:<10} {improved}\")\n",
    "    print(\"=\"*85)\n",
    "elif layernorm_results:\n",
    "    print(\"Original experiment results not found for comparison.\")\n",
    "else:\n",
    "    print(\"No LayerNorm results found yet. Run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Analyze Pre-activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Pre-activation Statistics (should be bounded with LayerNorm):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Actor LR':<10} {'Critic LR':<10} {'Min Preact':<15} {'Max Preact':<15} {'Saturation':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for exp_dir in sorted(os.listdir(results_dir)):\n",
    "        tracking_file = os.path.join(results_dir, exp_dir, 'tracking_data.json')\n",
    "        result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "        if os.path.exists(tracking_file) and os.path.exists(result_file):\n",
    "            with open(tracking_file) as f:\n",
    "                tracking = json.load(f)\n",
    "            with open(result_file) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            act_hist = tracking.get('activation_history', [])\n",
    "            if act_hist:\n",
    "                last = act_hist[-1]\n",
    "                print(f\"{result['actor_lr']:<10} {result['critic_lr']:<10} {last['min_preact']:<15.2f} {last['max_preact']:<15.2f} {last['avg_actor_output_saturation']:<15.2%}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNote: With LayerNorm, pre-activations should stay bounded (~-3 to +3)\")\n",
    "    print(\"Without LayerNorm, high LR experiments showed values in the MILLIONS!\")\n",
    "else:\n",
    "    print(\"No results found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "output_zip = '/content/layernorm_results.zip'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    shutil.make_archive('/content/layernorm_results', 'zip', results_dir)\n",
    "    print(f\"Created: {output_zip}\")\n",
    "    print(f\"Size: {os.path.getsize(output_zip) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Download\n",
    "    from google.colab import files\n",
    "    files.download(output_zip)\n",
    "else:\n",
    "    print(\"No results directory found.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}