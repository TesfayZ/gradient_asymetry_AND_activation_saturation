{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization Experiment - Testing if LayerNorm Prevents Saturation\n",
    "\n",
    "**Hypothesis:** LayerNorm normalizes pre-activations to ~0 mean, ~1 std before tanh, keeping outputs in the linear region where gradients flow properly.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "| Network | Hidden Activations | Pre-Output | Output Activation | Architecture |\n",
    "|---------|-------------------|------------|-------------------|---------------|\n",
    "| Actor   | ReLU | **LayerNorm** | tanh | 7->64->32->LN->tanh->3 |\n",
    "| Critic  | ReLU | None | linear | 510->512->128->1 |\n",
    "\n",
    "## Learning Rate Order (High to Low)\n",
    "Experiments run from **HIGH to LOW** learning rates to prioritize early-stopping cases:\n",
    "- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]`\n",
    "- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]`\n",
    "- Total: **16 experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Upload and Extract Experiment Files\n",
    "\n",
    "**Option A:** Upload `layernorm_experiment.zip` when prompted\n",
    "\n",
    "**Option B:** If you already uploaded to Drive, skip the upload prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if files already exist\n",
    "if os.path.exists('/content/layernorm_experiment/mec_env.py'):\n",
    "    print(\"Experiment files already present!\")\n",
    "else:\n",
    "    # Try to find zip in Drive first\n",
    "    drive_zip = '/content/drive/MyDrive/layernorm_experiment.zip'\n",
    "    \n",
    "    if os.path.exists(drive_zip):\n",
    "        print(f\"Found zip in Drive: {drive_zip}\")\n",
    "        zip_path = drive_zip\n",
    "    else:\n",
    "        # Upload zip file\n",
    "        print(\"Upload layernorm_experiment.zip:\")\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        zip_path = list(uploaded.keys())[0]\n",
    "    \n",
    "    # Extract\n",
    "    print(f\"Extracting {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall('/content')\n",
    "    \n",
    "    print(\"\\nExtracted files:\")\n",
    "    for item in os.listdir('/content'):\n",
    "        if not item.startswith('.'):\n",
    "            print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/content')\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify LayerNorm Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/layernorm_experiment')\n",
    "\n",
    "from Model import LayerNormActorNetwork, CriticNetwork, ActorNetwork\n",
    "import torch\n",
    "\n",
    "# Compare architectures\n",
    "original_actor = ActorNetwork(7, 3, torch.tanh)\n",
    "layernorm_actor = LayerNormActorNetwork(7, 3, torch.tanh)\n",
    "critic = CriticNetwork(350, 150, 7, 3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal Actor: 7 -> 64 -> 32 -> tanh -> 3\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in original_actor.parameters()):,}\")\n",
    "print(f\"\\nLayerNorm Actor: 7 -> 64 -> 32 -> LayerNorm -> tanh -> 3\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in layernorm_actor.parameters()):,}\")\n",
    "print(f\"\\nCritic: 510 -> 512 -> 128 -> 1 (linear)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
    "\n",
    "# Show the LayerNorm layer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LayerNorm Actor Modules:\")\n",
    "print(\"=\" * 60)\n",
    "for name, module in layernorm_actor.named_modules():\n",
    "    if name:\n",
    "        print(f\"  {name}: {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run All 16 Experiments\n",
    "\n",
    "This will run all 16 experiments with **HIGH learning rates FIRST**:\n",
    "- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "\n",
    "**Progress is auto-saved to Google Drive every 100 episodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "sys.path.insert(0, '/content/layernorm_experiment')\n",
    "\n",
    "from run_layernorm_experiment import run_all_experiments\n",
    "\n",
    "# Run all experiments\n",
    "run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Check Experiment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "status_file = os.path.join(results_dir, 'experiment_status.json')\n",
    "\n",
    "if os.path.exists(status_file):\n",
    "    with open(status_file) as f:\n",
    "        status = json.load(f)\n",
    "    print(\"Experiment Status:\")\n",
    "    print(f\"  Completed: {len(status['completed'])}/16\")\n",
    "    if status['in_progress']:\n",
    "        print(f\"  In progress: {status['in_progress']}\")\n",
    "    print(\"\\nCompleted experiments:\")\n",
    "    for exp in sorted(status['completed']):\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No status file found yet.\")\n",
    "\n",
    "# Also check Drive backup\n",
    "drive_dir = '/content/drive/MyDrive/layernorm_results'\n",
    "if os.path.exists(drive_dir):\n",
    "    print(f\"\\nDrive backup exists: {drive_dir}\")\n",
    "    contents = os.listdir(drive_dir)\n",
    "    print(f\"  Contents: {contents[:5]}...\" if len(contents) > 5 else f\"  Contents: {contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"LayerNorm Experiment Results Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Actor LR':<12} {'Critic LR':<12} {'Stop Ep.':<12} {'Final Reward':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for exp_dir in sorted(os.listdir(results_dir)):\n",
    "        result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "        if os.path.exists(result_file):\n",
    "            with open(result_file) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "    \n",
    "    for data in sorted(results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        print(f\"{data['actor_lr']:<12} {data['critic_lr']:<12} {data['stopping_episode']:<12} {data['final_reward']:<15.4f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No results directory found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Compare with Original (No LayerNorm) Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_results(results_dir):\n",
    "    results = []\n",
    "    if os.path.exists(results_dir):\n",
    "        for exp_dir in os.listdir(results_dir):\n",
    "            result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file) as f:\n",
    "                    results.append(json.load(f))\n",
    "    return results\n",
    "\n",
    "layernorm_results = load_results('/content/results/layernorm_experiment')\n",
    "original_results = load_results('/content/results/stopping_experiment')\n",
    "\n",
    "# Also try Drive backups\n",
    "if not original_results:\n",
    "    original_results = load_results('/content/drive/MyDrive/gradient_asymmetry_results')\n",
    "\n",
    "if original_results and layernorm_results:\n",
    "    print(\"COMPARISON: Original (No LayerNorm) vs LayerNorm Actor\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"{'Actor LR':<10} {'Critic LR':<10} {'Original':<15} {'LayerNorm':<15} {'Difference':<15}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    for orig in sorted(original_results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        ln = next((r for r in layernorm_results \n",
    "                   if r['actor_lr'] == orig['actor_lr'] and r['critic_lr'] == orig['critic_lr']), None)\n",
    "        if ln:\n",
    "            diff = ln['stopping_episode'] - orig['stopping_episode']\n",
    "            diff_str = f\"+{diff}\" if diff > 0 else str(diff)\n",
    "            improved = \"IMPROVED\" if diff > 100 else \"\"\n",
    "            print(f\"{orig['actor_lr']:<10} {orig['critic_lr']:<10} {orig['stopping_episode']:<15} {ln['stopping_episode']:<15} {diff_str:<10} {improved}\")\n",
    "    print(\"=\"*85)\n",
    "elif layernorm_results:\n",
    "    print(\"Original experiment results not found for comparison.\")\n",
    "else:\n",
    "    print(\"No LayerNorm results found yet. Run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Analyze Pre-activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Pre-activation Statistics (should be bounded with LayerNorm):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Actor LR':<10} {'Critic LR':<10} {'Min Preact':<15} {'Max Preact':<15} {'Saturation':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for exp_dir in sorted(os.listdir(results_dir)):\n",
    "        tracking_file = os.path.join(results_dir, exp_dir, 'tracking_data.json')\n",
    "        result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "        if os.path.exists(tracking_file) and os.path.exists(result_file):\n",
    "            with open(tracking_file) as f:\n",
    "                tracking = json.load(f)\n",
    "            with open(result_file) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            act_hist = tracking.get('activation_history', [])\n",
    "            if act_hist:\n",
    "                last = act_hist[-1]\n",
    "                print(f\"{result['actor_lr']:<10} {result['critic_lr']:<10} {last['min_preact']:<15.2f} {last['max_preact']:<15.2f} {last['avg_actor_output_saturation']:<15.2%}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNote: With LayerNorm, pre-activations should stay bounded (~-3 to +3)\")\n",
    "    print(\"Without LayerNorm, high LR experiments showed values in the MILLIONS!\")\n",
    "else:\n",
    "    print(\"No results found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/layernorm_experiment'\n",
    "output_zip = '/content/layernorm_results.zip'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    shutil.make_archive('/content/layernorm_results', 'zip', results_dir)\n",
    "    print(f\"Created: {output_zip}\")\n",
    "    print(f\"Size: {os.path.getsize(output_zip) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Download\n",
    "    from google.colab import files\n",
    "    files.download(output_zip)\n",
    "else:\n",
    "    print(\"No results directory found.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
