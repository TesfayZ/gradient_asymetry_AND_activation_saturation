{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Large Actor Experiment - Testing if Larger Architecture Prevents Saturation\n\n**Hypothesis:** Making the actor have **DOUBLE** the hidden layer sizes of the critic (1024 -> 256 instead of 64 -> 32) may help avoid tanh saturation by distributing learning signal across more parameters.\n\n## Architecture Comparison\n\n| Network | Original | Large Actor (This Experiment) |\n|---------|----------|-------------------------------|\n| Actor   | 7->64->32->3 | 7->**1024**->**256**->3 (2x critic) |\n| Critic  | 510->512->128->1 | 510->512->128->1 |\n\n## Learning Rate Order (High to Low)\nExperiments run from **HIGH to LOW** learning rates to prioritize early-stopping cases:\n- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]`\n- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]`\n- Total: **16 experiments**\n\n---\n\n## Features:\n- Auto-saves to Google Drive every 100 episodes\n- Auto-restores from Drive on session restart\n- Crash recovery built-in\n- Early stopping when all actors stop updating\n- **Seed = 42** for complete reproducibility"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Setup - Choose Start Mode\n\nThe experiment script **automatically restores from Google Drive** when you run it. Choose how to proceed:\n\n- **Option 1: Continue** - Run experiments (auto-restores completed ones from Drive)\n- **Option 2: Start Fresh** - Clear ALL results (local + Drive) and start from scratch  \n- **Option 3: Import from Zip** - Import results from a downloaded zip file"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport zipfile\nimport shutil\nimport json\n\nEXPERIMENT_NAME = 'large_actor_experiment'\nEXPERIMENT_ZIP = f'{EXPERIMENT_NAME}.zip'\nRESULTS_DIR = f'/content/results/{EXPERIMENT_NAME}'\nEXPERIMENT_DIR = f'/content/{EXPERIMENT_NAME}'\nDRIVE_BACKUP_DIR = '/content/drive/MyDrive/large_actor_results'\n\n# ============================================================\n# STEP 1: Extract experiment files\n# ============================================================\nif not os.path.exists(f'{EXPERIMENT_DIR}/mec_env.py'):\n    drive_zip = f'/content/drive/MyDrive/{EXPERIMENT_ZIP}'\n    \n    if os.path.exists(drive_zip):\n        print(f\"Found experiment zip in Drive: {drive_zip}\")\n        zip_path = drive_zip\n    else:\n        print(f\"Upload {EXPERIMENT_ZIP}:\")\n        from google.colab import files\n        uploaded = files.upload()\n        zip_path = list(uploaded.keys())[0]\n    \n    print(f\"Extracting {zip_path}...\")\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        z.extractall('/content')\n    print(\"Experiment files extracted!\")\nelse:\n    print(\"Experiment files already present!\")\n\n# ============================================================\n# STEP 2: Check existing progress\n# ============================================================\ndrive_status_file = os.path.join(DRIVE_BACKUP_DIR, 'experiment_status.json')\nif os.path.exists(drive_status_file):\n    with open(drive_status_file) as f:\n        drive_status = json.load(f)\n    drive_completed = len(drive_status.get('completed', []))\n    in_progress = drive_status.get('in_progress')\n    print(f\"\\n*** Found Drive backup: {drive_completed}/16 completed ***\")\n    if in_progress:\n        print(f\"    Last in progress: {in_progress} (will restart this one)\")\nelse:\n    drive_completed = 0\n    print(\"\\n*** No existing Drive backup found ***\")\n\n# ============================================================\n# CHOOSE START MODE\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SELECT START MODE\")\nprint(\"=\" * 60)\nprint(\"1. Continue - Auto-restore from Drive and run remaining\")\nprint(\"2. Start Fresh - Clear ALL results and start from scratch\")\nprint(\"3. Import from Zip - Upload results zip to restore\")\nprint(\"=\" * 60)\n\nstart_mode = input(\"Enter choice (1/2/3): \").strip()\n\nif start_mode == '2':\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STARTING FRESH - Clearing all results\")\n    print(\"=\" * 60)\n    if os.path.exists(RESULTS_DIR):\n        shutil.rmtree(RESULTS_DIR)\n        print(f\"Cleared: {RESULTS_DIR}\")\n    if os.path.exists(DRIVE_BACKUP_DIR):\n        shutil.rmtree(DRIVE_BACKUP_DIR)\n        print(f\"Cleared: {DRIVE_BACKUP_DIR}\")\n    print(\"Ready to run all 16 experiments from scratch!\")\n\nelif start_mode == '3':\n    print(\"\\n\" + \"=\" * 60)\n    print(\"IMPORT FROM ZIP\")\n    print(\"=\" * 60)\n    \n    print(\"Upload your results zip file:\")\n    from google.colab import files\n    uploaded = files.upload()\n    results_zip_path = list(uploaded.keys())[0]\n    \n    temp_dir = '/content/temp_import'\n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir)\n    os.makedirs(temp_dir)\n    \n    with zipfile.ZipFile(results_zip_path, 'r') as z:\n        z.extractall(temp_dir)\n    \n    actual_results = temp_dir\n    for root, dirs, files_list in os.walk(temp_dir):\n        if 'experiment_status.json' in files_list:\n            actual_results = root\n            break\n    \n    if os.path.exists(DRIVE_BACKUP_DIR):\n        shutil.rmtree(DRIVE_BACKUP_DIR)\n    shutil.copytree(actual_results, DRIVE_BACKUP_DIR)\n    \n    with open(os.path.join(DRIVE_BACKUP_DIR, 'experiment_status.json')) as f:\n        imported_status = json.load(f)\n    print(f\"Imported {len(imported_status.get('completed', []))} completed experiments to Drive\")\n    print(\"The experiment script will auto-restore these when you run it.\")\n    \n    shutil.rmtree(temp_dir)\n\nelse:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONTINUE MODE\")\n    print(\"=\" * 60)\n    if drive_completed > 0:\n        print(f\"Will auto-restore {drive_completed} completed experiments from Drive\")\n    print(\"Ready to run!\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SETUP COMPLETE - Run the next cell to start experiments\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime -> Change runtime type -> GPU\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/content')\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify Large Actor Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content/large_actor_experiment')\n\nfrom Model import LargeActorNetwork, ActorNetwork, CriticNetwork\nimport torch\n\n# Compare architectures\nsmall_actor = ActorNetwork(7, 3, torch.tanh)\nlarge_actor = LargeActorNetwork(7, 3, torch.tanh)\ncritic = CriticNetwork(350, 150, 7, 3)  # 50 agents * 7 state, 50 agents * 3 action\n\nprint(\"=\" * 50)\nprint(\"ARCHITECTURE COMPARISON\")\nprint(\"=\" * 50)\nprint(f\"\\nSmall Actor (original): 7 -> 64 -> 32 -> 3\")\nprint(f\"  Parameters: {sum(p.numel() for p in small_actor.parameters()):,}\")\nprint(f\"\\nLarge Actor (this experiment): 7 -> 1024 -> 256 -> 3 (2x critic)\")\nprint(f\"  Parameters: {sum(p.numel() for p in large_actor.parameters()):,}\")\nprint(f\"\\nCritic: 510 -> 512 -> 128 -> 1\")\nprint(f\"  Parameters: {sum(p.numel() for p in critic.parameters()):,}\")\nprint(f\"\\nLarge Actor / Critic ratio: {sum(p.numel() for p in large_actor.parameters()) / sum(p.numel() for p in critic.parameters()):.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run All 16 Experiments\n",
    "\n",
    "This will run all 16 experiments with **HIGH learning rates FIRST**:\n",
    "- Actor LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "- Critic LRs: `[0.1, 0.01, 0.001, 0.0001]` (high to low)\n",
    "\n",
    "**Progress is auto-saved to Google Drive every 100 episodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "sys.path.insert(0, '/content/large_actor_experiment')\n",
    "\n",
    "from run_large_actor_experiment import run_all_experiments\n",
    "\n",
    "# Run all experiments\n",
    "run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Check Experiment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/large_actor_experiment'\n",
    "status_file = os.path.join(results_dir, 'experiment_status.json')\n",
    "\n",
    "if os.path.exists(status_file):\n",
    "    with open(status_file) as f:\n",
    "        status = json.load(f)\n",
    "    print(\"Experiment Status:\")\n",
    "    print(f\"  Completed: {len(status['completed'])}/16\")\n",
    "    if status['in_progress']:\n",
    "        print(f\"  In progress: {status['in_progress']}\")\n",
    "    print(\"\\nCompleted experiments:\")\n",
    "    for exp in sorted(status['completed']):\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No status file found yet.\")\n",
    "\n",
    "# Also check Drive backup\n",
    "drive_dir = '/content/drive/MyDrive/large_actor_results'\n",
    "if os.path.exists(drive_dir):\n",
    "    print(f\"\\nDrive backup exists: {drive_dir}\")\n",
    "    contents = os.listdir(drive_dir)\n",
    "    print(f\"  Contents: {contents[:5]}...\" if len(contents) > 5 else f\"  Contents: {contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/large_actor_experiment'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Large Actor Experiment Results Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Actor LR':<12} {'Critic LR':<12} {'Stop Ep.':<12} {'Final Reward':<15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for exp_dir in sorted(os.listdir(results_dir)):\n",
    "        result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "        if os.path.exists(result_file):\n",
    "            with open(result_file) as f:\n",
    "                data = json.load(f)\n",
    "            results.append(data)\n",
    "    \n",
    "    # Sort by actor_lr (descending) then critic_lr (descending)\n",
    "    for data in sorted(results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        print(f\"{data['actor_lr']:<12} {data['critic_lr']:<12} {data['stopping_episode']:<12} {data['final_reward']:<15.4f}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No results directory found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Compare with Original Experiment (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_results(results_dir):\n",
    "    results = []\n",
    "    if os.path.exists(results_dir):\n",
    "        for exp_dir in os.listdir(results_dir):\n",
    "            result_file = os.path.join(results_dir, exp_dir, 'results.json')\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file) as f:\n",
    "                    results.append(json.load(f))\n",
    "    return results\n",
    "\n",
    "large_results = load_results('/content/results/large_actor_experiment')\n",
    "original_results = load_results('/content/results/stopping_experiment')\n",
    "\n",
    "# Also try Drive backups\n",
    "if not original_results:\n",
    "    original_results = load_results('/content/drive/MyDrive/gradient_asymmetry_results')\n",
    "\n",
    "if original_results and large_results:\n",
    "    print(\"COMPARISON: Original (Small Actor) vs Large Actor\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"{'Actor LR':<10} {'Critic LR':<10} {'Original Stop':<15} {'Large Stop':<15} {'Difference':<15}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    for orig in sorted(original_results, key=lambda x: (-x['actor_lr'], -x['critic_lr'])):\n",
    "        large = next((r for r in large_results \n",
    "                     if r['actor_lr'] == orig['actor_lr'] and r['critic_lr'] == orig['critic_lr']), None)\n",
    "        if large:\n",
    "            diff = large['stopping_episode'] - orig['stopping_episode']\n",
    "            diff_str = f\"+{diff}\" if diff > 0 else str(diff)\n",
    "            improved = \"IMPROVED\" if diff > 100 else \"\"\n",
    "            print(f\"{orig['actor_lr']:<10} {orig['critic_lr']:<10} {orig['stopping_episode']:<15} {large['stopping_episode']:<15} {diff_str:<10} {improved}\")\n",
    "    print(\"=\"*85)\n",
    "elif large_results:\n",
    "    print(\"Original experiment results not found for comparison.\")\n",
    "    print(\"Upload original results to '/content/results/stopping_experiment' to compare.\")\n",
    "else:\n",
    "    print(\"No large actor results found yet. Run the experiment first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Download Results (Optional)\n",
    "\n",
    "Results are auto-backed up to Drive, but you can also download a zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "results_dir = '/content/results/large_actor_experiment'\n",
    "output_zip = '/content/large_actor_results.zip'\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    shutil.make_archive('/content/large_actor_results', 'zip', results_dir)\n",
    "    print(f\"Created: {output_zip}\")\n",
    "    print(f\"Size: {os.path.getsize(output_zip) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Download\n",
    "    from google.colab import files\n",
    "    files.download(output_zip)\n",
    "else:\n",
    "    print(\"No results directory found.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}