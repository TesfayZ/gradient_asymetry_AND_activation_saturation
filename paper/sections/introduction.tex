\section{Introduction}
\label{sec:introduction}

\subsection{Background and Motivation}

Deep reinforcement learning (DRL) has achieved remarkable success in solving complex sequential decision-making problems, from game playing to robotic control. The extension to multi-agent settings, known as multi-agent deep reinforcement learning (MADRL), addresses scenarios where multiple agents must learn to coordinate or compete in shared environments. Actor-critic architectures, particularly the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm~\cite{lowe2017multi}, have become foundational approaches for continuous control in multi-agent systems.

In actor-critic methods, the actor network learns a policy that maps states to actions, while the critic network estimates the value of state-action pairs to guide policy improvement. This architectural separation creates an inherent asymmetry in how gradients flow through the system during training. The critic receives direct supervision from temporal difference (TD) errors, while the actor receives indirect feedback through the critic's value estimates.

Despite the widespread adoption of actor-critic MADRL, a systematic understanding of how this architectural asymmetry affects learning dynamics---particularly the differential convergence behavior between actors and critics---remains incomplete. This gap is especially significant when considering hyperparameter selection, as practitioners often observe that actors and critics exhibit different sensitivities to learning rate choices~\cite{henderson2018deep}.

\subsection{Problem Statement}

In our prior work on the Client-Master MADRL framework (CCM-MADRL) for MEC task offloading~\cite{ccm_madrl, gebrekidan2024thesis}, we conducted hyperparameter tuning experiments involving 16 combinations of learning rates. During this investigation, we observed a striking phenomenon: \textbf{client agents (actors) exhibited weight update cessation}---their neural network parameters stopped changing within $\sim$5 episodes under high learning rate configurations, while the master agent (critic) continued learning until training completion. Only 2 of 16 learning rate combinations produced stable actor learning throughout the full 2000 episodes. Figure~\ref{fig:thesis_stopping} illustrates this phenomenon from our original experiments.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig0_thesis_stopping.png}
\Description{Bubble plot from thesis experiments showing stopping episodes. High actor learning rates (0.01, 0.1) result in stopping at exactly 5 episodes across all critic LR values. Lower actor LRs (0.0001, 0.001) show varied stopping times from 27 to 1999 episodes, with the lowest actor LR achieving near-complete training.}
\caption{Original thesis results~\cite{gebrekidan2024thesis}: Stopping episodes across 16 learning rate combinations. Bubble size indicates episodes before actors stop updating. High actor learning rates ($\geq$0.01) cause immediate stopping at $\sim$5 episodes (rightmost columns), while lower rates allow extended training. This observation motivated the current investigation.}
\label{fig:thesis_stopping}
\end{figure}

\textit{Terminology:} Throughout this paper, we use ``weight update cessation'' or ``stopping'' to describe the phenomenon where gradient updates become negligible (parameter changes $<10^{-8}$) due to activation saturation, distinct from intentional early stopping or convergence to an optimum.

While our prior work identified the optimal learning rate configurations empirically, the \textit{underlying mechanism} causing this asymmetric behavior remained unexplained. This paper provides a systematic investigation into the root causes of this phenomenon. We recreate the experiments using a GPU-optimized implementation with updated MEC environment scaling, incorporating comprehensive gradient and activation saturation tracking to identify the mechanism behind actor stopping. Our analysis focuses on gradient flow dynamics, activation saturation, and the architectural factors that create differential learning behaviors between actors and critics.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Empirical characterization} of the differential stopping phenomenon in Client-Master MADRL, demonstrating that actors and critics exhibit fundamentally different convergence behaviors across learning rate configurations.

    \item \textbf{Theoretical analysis} identifying tanh output activation saturation as the primary mechanism causing premature actor convergence, explaining the learning-rate-dependent nature of this phenomenon.

    \item \textbf{Systematic investigation} of the architectural and gradient flow factors that create asymmetry between actor and critic learning dynamics.

    \item \textbf{Practical guidelines} for learning rate selection and network design to prevent premature actor convergence while maintaining stable critic learning.

    \item \textbf{Experimental framework} for detecting and monitoring gradient flow asymmetries during MADRL training.
\end{enumerate}

\subsection{Paper Organization}

Section~\ref{sec:related} reviews related work on actor-critic methods, gradient flow in deep learning, and hyperparameter sensitivity in DRL. Section~\ref{sec:background} presents the technical background and problem formulation. Section~\ref{sec:methodology} details our methodology and experimental setup. Section~\ref{sec:analysis} presents our analysis and findings. Section~\ref{sec:discussion} discusses implications and proposed solutions. Section~\ref{sec:conclusion} concludes the paper.
