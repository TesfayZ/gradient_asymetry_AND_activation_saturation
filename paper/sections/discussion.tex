\section{Discussion}
\label{sec:discussion}

\subsection{Theoretical Implications}

Our findings reveal a \textbf{fundamental tension} in actor-critic design for continuous control:

\begin{enumerate}
    \item \textbf{Bounded outputs are necessary} for valid action generation in continuous spaces
    \item \textbf{Bounded activations (tanh) are susceptible} to saturation and gradient vanishing
    \item \textbf{Large reward scales and high learning rates} exacerbate saturation
    \item \textbf{Critics with linear outputs} are immune to this specific pathology
\end{enumerate}

This creates an inherent asymmetry: critics can use aggressive learning rates, while actors require conservative rates to avoid saturation.

\subsection{The Learning Rate Paradox}

The $10^6\times$ gradient asymmetry creates a paradox where \textbf{no actor learning rate is optimal}:

\textbf{Low Actor LR ($\eta_a \leq 0.0001$):}
\begin{itemize}
    \item Actor updates are too small relative to critic's rapid Q-landscape changes
    \item The actor ``chases a moving target'' -- by the time it adapts to the current Q-function, the critic has already shifted
    \item Result: Slow convergence, suboptimal policies
\end{itemize}

\textbf{High Actor LR ($\eta_a \geq 0.01$):}
\begin{itemize}
    \item Large weight updates push pre-activation values into saturation regions
    \item tanh outputs lock at $\pm 1$, gradients vanish
    \item Result: Actor stops learning entirely within a few episodes
\end{itemize}

\textbf{The ``Chasing a Moving Target'' Dynamic:}

The critic learns $10^6\times$ faster than the actor due to gradient magnitude differences. This creates a non-stationary optimization landscape for the actor:

\begin{equation}
\frac{\partial Q}{\partial \theta_c} \gg \frac{\partial \mathcal{L}_a}{\partial \theta_a} \implies \Delta \theta_c \gg \Delta \theta_a
\end{equation}

The actor's policy $\pi_{\theta_a}$ is optimized against $Q_{\theta_c}$, but $\theta_c$ changes substantially between actor updates. The actor effectively optimizes against a stale Q-function, leading to:
\begin{enumerate}
    \item Oscillating policies that never stabilize
    \item Suboptimal convergence to local minima
    \item Increased sensitivity to learning rate selection
\end{enumerate}

This explains why the recommended actor/critic LR ratio of $\sim$0.1 (actor LR $<$ critic LR) works empirically -- it partially compensates for the gradient magnitude imbalance, though it cannot fully resolve the non-stationarity issue.

\subsection{Statistical Validation: Only 1 of 16 Configurations Converge}

The severity of the learning rate sensitivity is underscored by our original thesis experiments~\cite{gebrekidan2024thesis}, which conducted 10 independent runs per configuration with 95\% confidence intervals. Remarkably, \textbf{only 1 of 16 learning rate combinations consistently achieved policy improvement}: actor LR = 0.0001 with critic LR = 0.001. This finding is replicated in our current experiments, as evidenced by Figure~\ref{fig:reward_comparison}. This optimal configuration falls precisely within the narrow window predicted by our analysis:
\begin{itemize}
    \item Actor LR = 0.0001 is conservative enough to avoid saturation (pre-activations remain bounded)
    \item Critic LR = 0.001 provides sufficient learning speed while maintaining stability
    \item The ratio $\eta_a / \eta_c = 0.1$ compensates for the gradient magnitude asymmetry
\end{itemize}

All 15 other configurations exhibited either:
\begin{enumerate}
    \item \textbf{Premature stopping} (actor LR $\geq$ 0.01): Tanh saturation within 300 episodes
    \item \textbf{High variance} (actor LR = 0.0001, critic LR = 0.0001): Too slow to converge reliably within 2,000 episodes
    \item \textbf{Instability} (actor LR = 0.001, critic LR = 0.1): Critic changes too rapidly for actor to track
\end{enumerate}

This statistical validation confirms that the gradient asymmetry creates an extremely narrow region of viable hyperparameters, making MADDPG-style algorithms fragile in practice without the mitigation strategies discussed below.

\subsection{Practical Recommendations}

Based on our analysis, we recommend:

\textbf{Learning Rate Selection:}
\begin{itemize}
    \item Actor learning rate: 0.0001 - 0.001 (conservative)
    \item Critic learning rate: 0.001 - 0.01 (can be 10$\times$ higher than actor)
    \item Recommended ratio: Actor LR / Critic LR $\approx$ 0.1
\end{itemize}

\textbf{Reward Scaling:}
\begin{itemize}
    \item Normalize rewards to unit scale when possible
    \item Monitor effective learning rate = $\eta \times |R|$
    \item Keep effective LR $<$ 0.1 for actors
\end{itemize}

\textbf{Architecture Modifications:}
\begin{itemize}
    \item Consider replacing tanh with scaled sigmoid or softsign
    \item Add gradient clipping on actor updates
    \item Monitor pre-activation magnitudes during training
\end{itemize}

\textbf{Training Monitoring:}
\begin{itemize}
    \item Implement weight change detection for actors
    \item Track tanh output distribution (warning if clustered at $\pm 1$)
    \item Compare actor vs critic gradient magnitudes
\end{itemize}

\subsection{Analysis from Multiple Perspectives}

To ensure comprehensive understanding, we analyze our findings from multiple theoretical perspectives.

\subsubsection{Optimization Perspective}

From an optimization standpoint, the actor and critic are jointly solving a bilevel optimization problem where the actor optimizes policy performance given the critic's value estimates, while the critic learns to accurately predict returns. The gradient asymmetry we observe implies that this joint optimization is inherently unbalanced---the critic's optimization landscape changes faster than the actor can track, violating the quasi-static assumption that underlies many convergence proofs for actor-critic methods~\cite{konda2000actor}.

\subsubsection{Information-Theoretic Perspective}

From an information flow perspective, the critic receives direct reward feedback (high information bandwidth), while the actor receives reward information only indirectly through the critic's Q-value gradients (low information bandwidth). The tanh saturation further reduces this bandwidth by compressing the gradient signal. This information bottleneck explains why actors are more sensitive to hyperparameter choices---they operate with less margin for error in extracting learning signal from their limited information channel.

\subsubsection{Stability Analysis Perspective}

The differential convergence behavior can be viewed through the lens of dynamical systems stability. The actor-critic system has two coupled dynamical subsystems with different time constants. When the actor's effective time constant (inverse learning rate) is too small relative to the critic's, the system enters an unstable regime where the actor overshoots, saturates, and loses the ability to track the critic's changes. Conservative actor learning rates increase the actor's time constant, improving stability at the cost of slower adaptation.

\subsubsection{Robustness of CCM-MADRL Despite Actor Freezing}

An important observation is that CCM-MADRL continues to improve performance even after actors stop updating. This robustness stems from the master agent's selection mechanism: even with fixed actor policies, the critic continues refining Q-value estimates, enabling better selection of which agents to include in the offloading coalition. This architectural feature provides partial immunity to actor stagnation but does not eliminate the underlying gradient pathology.

\subsection{Broader Impact}

This analysis has implications beyond MEC task offloading:

\begin{enumerate}
    \item \textbf{DDPG/TD3/SAC implementations:} All use tanh-bounded outputs and may exhibit similar behavior. TD3's delayed policy updates~\cite{fujimoto2018addressing} may partially mitigate this by reducing actor update frequency, implicitly increasing the effective actor time constant.

    \item \textbf{Hyperparameter transfer:} Learning rates optimal in one domain may fail in others with different reward scales. Our analysis suggests that practitioners should normalize rewards or adjust learning rates proportionally when transferring hyperparameters across domains.

    \item \textbf{Multi-agent systems:} Shared rewards amplify the effective reward scale ($N$ agents $\times$ per-agent contribution), making MARL systems particularly susceptible to the saturation effects we characterize.

    \item \textbf{Reward shaping:} Dense reward shaping, often used to accelerate learning, can inadvertently increase reward magnitude and trigger premature actor saturation if learning rates are not correspondingly reduced.
\end{enumerate}

\subsection{Mitigation Strategies from the Literature}

Several techniques in the literature address aspects of the gradient asymmetry and saturation problems we identify. We categorize these by their mechanism of action.

\subsubsection{Implicit Learning Rate Adjustment}

\begin{itemize}
    \item \textbf{Delayed policy updates (TD3)~\cite{fujimoto2018addressing}:} By updating the actor less frequently than the critic (typically every 2 critic updates), TD3 effectively halves the actor's learning rate relative to the critic. This aligns with our recommendation for conservative actor learning rates and partially compensates for gradient magnitude asymmetry.

    \item \textbf{Target networks~\cite{lillicrap2015continuous}:} Target networks stabilize critic learning by providing slowly-moving Q-targets, reducing the non-stationarity that actors must track. However, this does not directly address actor saturation.
\end{itemize}

\subsubsection{Alternative Action Parameterizations}

These approaches avoid tanh saturation by using different output distributions:

\begin{itemize}
    \item \textbf{Squashed Gaussian (SAC)~\cite{haarnoja2018soft}:} SAC parameterizes the policy as a Gaussian distribution over unbounded actions, then applies tanh squashing \textit{after} sampling. Crucially, gradients flow through the unbounded mean and standard deviation parameters, not through the tanh function. The log-probability correction $\log \pi(a|s) = \log \mu(u|s) - \sum_i \log(1 - \tanh^2(u_i))$ accounts for the squashing without creating gradient bottlenecks. This is the most direct solution to tanh saturation in the literature.

    \item \textbf{Beta distribution~\cite{chou2017improving}:} Instead of squashing unbounded outputs, this approach uses the Beta distribution which naturally produces bounded outputs in $[0, 1]$. The policy outputs shape parameters $\alpha, \beta > 0$, and actions are sampled directly from $\text{Beta}(\alpha, \beta)$. This eliminates the saturating activation entirely while maintaining valid bounded actions.
\end{itemize}

\subsubsection{Normalization Techniques}

\begin{itemize}
    \item \textbf{Reward normalization~\cite{henderson2018deep}:} Normalizing rewards to zero mean and unit variance reduces the effective learning rate ($\eta \times |R|$), preventing the large weight updates that cause saturation. This is particularly important in multi-agent settings where reward magnitudes scale with agent count.

    \item \textbf{Layer normalization~\cite{ba2016layer}:} Applying layer normalization before the tanh output keeps pre-activation values bounded, preventing extreme inputs that cause saturation. Unlike batch normalization~\cite{ioffe2015batch}, layer normalization operates per-sample and is suitable for RL where batch statistics are non-stationary.
\end{itemize}

\subsubsection{Training Procedure Modifications}

\begin{itemize}
    \item \textbf{Phasic policy gradient (PPG)~\cite{cobbe2021phasic}:} PPG separates policy and value function training into distinct phases, reducing gradient interference between actor and critic. This addresses the ``chasing a moving target'' dynamic by allowing the critic to stabilize before actor updates.

    \item \textbf{Entropy regularization (SAC)~\cite{haarnoja2018soft}:} The entropy bonus $\mathcal{H}[\pi(\cdot|s)]$ encourages policy diversity, potentially preventing early convergence to deterministic (saturated) outputs. However, this is an indirect mitigation that does not address gradient magnitude asymmetry.

    \item \textbf{Gradient clipping~\cite{pascanu2013difficulty}:} While gradient clipping can prevent exploding gradients in critics, it does not address the \textit{vanishing} gradient problem in actors caused by tanh saturation---clipping reduces large gradients but cannot amplify small ones.
\end{itemize}

\subsubsection{Summary and Recommendations}

Table~\ref{tab:mitigations} summarizes the mitigation strategies and their effectiveness against our identified causes.

\begin{table}[htbp]
\caption{Mitigation Strategies vs. Causes of Gradient Asymmetry}
\label{tab:mitigations}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Mitigation} & \textbf{Loss} & \textbf{Path} & \textbf{Activation} \\
 & \textbf{Asymm.} & \textbf{Length} & \textbf{Saturation} \\
\midrule
Delayed updates (TD3) & \ding{51} & -- & -- \\
Squashed Gaussian (SAC) & -- & -- & \ding{51}\ding{51} \\
Beta distribution & -- & -- & \ding{51}\ding{51} \\
Reward normalization & \ding{51} & -- & \ding{51} \\
Layer normalization & -- & -- & \ding{51} \\
Phasic training (PPG) & -- & \ding{51} & -- \\
Entropy regularization & -- & -- & \ding{51} \\
\bottomrule
\multicolumn{4}{l}{\footnotesize \ding{51}\ding{51} = directly addresses; \ding{51} = partially addresses; -- = does not address}
\end{tabular}
\end{table}

Based on our analysis, the most effective mitigations for tanh saturation are \textbf{Squashed Gaussian} and \textbf{Beta distribution} parameterizations, as they bypass the saturating activation entirely. For practitioners using deterministic policies (DDPG/TD3), combining \textbf{reward normalization} with \textbf{layer normalization} before the tanh output provides a practical alternative without changing the policy structure.

\subsection{Summary of Evidence}

Our comprehensive experiments validate the theoretical analysis through multiple lines of evidence:

\begin{enumerate}
    \item \textbf{Gradient asymmetry:} Measured ratio $\rho \sim 10^{-8}$ to $10^{-4}$ confirms 4--8 orders of magnitude critic gradient dominance across all configurations.

    \item \textbf{Saturation correlation:} Strong correlation ($r > 0.9$) between final saturation ratio and stopping episode demonstrates the causal chain from high LR to saturation to weight update cessation.

    \item \textbf{Layer-wise analysis:} Gradient breakdown occurs specifically at the actor output layer (tanh), while hidden layers (ReLU) maintain gradient flow.

    \item \textbf{Consistency:} The pattern holds across all 16 learning rate configurations, with actor LR as the dominant factor and critic LR having secondary effect only at extreme values (0.1).
\end{enumerate}

These findings provide both theoretical explanation and empirical validation for the gradient asymmetry phenomenon in actor-critic architectures.

\subsection{Network Capacity Does Not Prevent Saturation}

A natural hypothesis is that increasing actor network capacity might prevent saturation by distributing the learning signal across more parameters and producing smaller pre-activation values. To test this, we conducted an ablation study where the actor hidden layers were increased from $(64 \rightarrow 32)$ to $(512 \rightarrow 128)$---matching the critic's hidden layer dimensions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/large_actor_comparison.pdf}
    \caption{Comparison of stopping episodes between small actor (64$\rightarrow$32, $\sim$2,500 parameters) and large actor (512$\rightarrow$128, $\sim$70,000 parameters). Despite 28$\times$ more parameters, the large actor still exhibits early stopping due to saturation. The green dashed line indicates the maximum training duration (2,000 episodes).}
    \label{fig:large_actor}
\end{figure}

Table~\ref{tab:large_actor} summarizes the results for high learning rate configurations where early stopping was observed.

\begin{table}[htbp]
\caption{Stopping Episodes: Small vs Large Actor Architecture}
\label{tab:large_actor}
\centering
\small
\begin{tabular}{cccc}
\toprule
\textbf{Actor LR} & \textbf{Critic LR} & \textbf{Small (64$\rightarrow$32)} & \textbf{Large (512$\rightarrow$128)} \\
\midrule
0.1 & 0.1 & 161 & 161 \\
0.1 & 0.01 & 176 & 194 \\
0.1 & 0.001 & 217 & 245 \\
0.1 & 0.0001 & 215 & 238 \\
0.01 & 0.1 & 247 & 253 \\
0.01 & 0.01 & 233 & 238 \\
0.01 & 0.001 & 233 & 246 \\
0.01 & 0.0001 & 233 & 240 \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that \textbf{increasing actor network capacity does not prevent tanh saturation}. The large actor (with 28$\times$ more parameters) shows nearly identical stopping episodes, with only marginal improvements of 5--28 episodes in some configurations. Critically, all high learning rate experiments still stopped well before 300 episodes out of 2,000.

This finding confirms that the saturation problem is \textit{architectural} rather than \textit{capacity-limited}:
\begin{itemize}
    \item The bottleneck is the \textbf{tanh output activation}, not the hidden layer capacity
    \item Larger networks can still produce extreme pre-activation values that saturate tanh
    \item More parameters may even accelerate saturation by enabling larger weight updates
\end{itemize}

This result reinforces our recommendation that effective mitigations must address the output activation itself (e.g., Squashed Gaussian, Beta distribution) rather than simply increasing network size.

\subsection{Linear Hidden Activations Exacerbate the Problem}

A natural hypothesis is that ReLU activations in hidden layers might contribute to saturation by producing only positive values, potentially misaligned with negative reward signals. To test this, we replaced ReLU with linear (identity) activations in both actor and critic hidden layers, allowing the network to naturally represent negative values throughout computation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/linear_activation_comparison.pdf}
    \caption{Comparison of stopping episodes between ReLU and linear hidden layer activations. Despite removing nonlinearity from hidden layers, early stopping still occurs. The green dashed line indicates full training duration (2,000 episodes).}
    \label{fig:linear_activation}
\end{figure}

Table~\ref{tab:linear_activation} compares the results.

\begin{table}[htbp]
\caption{Stopping Episodes: ReLU vs Linear Hidden Activations}
\label{tab:linear_activation}
\centering
\small
\begin{tabular}{cccc}
\toprule
\textbf{Actor LR} & \textbf{Critic LR} & \textbf{ReLU Hidden} & \textbf{Linear Hidden} \\
\midrule
0.1 & 0.1 & 161 & 301 \\
0.1 & 0.01 & 176 & 251 \\
0.1 & 0.001 & 217 & 250 \\
0.1 & 0.0001 & 215 & 269 \\
0.01 & 0.1 & 247 & 307 \\
0.01 & 0.01 & 233 & 307 \\
0.01 & 0.001 & 233 & 286 \\
0.01 & 0.0001 & 233 & 276 \\
\bottomrule
\end{tabular}
\end{table}

Surprisingly, linear hidden activations show \textit{slightly delayed} stopping (33--140 additional episodes), but \textbf{pre-activation explosion is dramatically worse}:

\begin{table}[htbp]
\caption{Pre-activation Magnitudes: ReLU vs Linear Hidden Layers}
\label{tab:linear_preact}
\centering
\small
\begin{tabular}{ccc}
\toprule
\textbf{Actor LR} & \textbf{ReLU Pre-activation Range} & \textbf{Linear Pre-activation Range} \\
\midrule
0.1 & $\pm 1.07 \times 10^{6}$ & $\pm 1.83 \times 10^{7}$ \\
0.01 & $\pm 5.2 \times 10^{3}$ & $\pm 8.1 \times 10^{4}$ \\
\bottomrule
\end{tabular}
\end{table}

With linear hidden layers, pre-activations explode to \textbf{18 million} (vs 1 million with ReLU)---a 17$\times$ increase. This occurs because:
\begin{itemize}
    \item ReLU provides implicit regularization by clipping negative values
    \item Without nonlinearity, the network is effectively a single deep linear transformation
    \item Large weights propagate directly through all layers without any dampening
\end{itemize}

This experiment conclusively demonstrates that the saturation problem originates at the \textbf{tanh output layer}, not the hidden layer activations. Modifying hidden layers---whether by changing size or activation function---cannot address the fundamental issue of bounded output activations combined with large learning rates.
