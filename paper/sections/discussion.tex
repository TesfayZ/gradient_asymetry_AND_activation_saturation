\section{Discussion}
\label{sec:discussion}

\subsection{Theoretical Implications}

Our findings reveal a \textbf{fundamental tension} in actor-critic design for continuous control:

\begin{enumerate}
    \item \textbf{Bounded outputs are necessary} for valid action generation in continuous spaces
    \item \textbf{Bounded activations (tanh) are susceptible} to saturation and gradient vanishing
    \item \textbf{Large reward scales and high learning rates} exacerbate saturation
    \item \textbf{Critics with linear outputs} are immune to this specific pathology
\end{enumerate}

This creates an inherent asymmetry: critics can use aggressive learning rates, while actors require conservative rates to avoid saturation.

\subsection{The Learning Rate Paradox}

The $10^6\times$ gradient asymmetry creates a paradox where \textbf{no actor learning rate is optimal}:

\textbf{Low Actor LR ($\eta_a \leq 0.0001$):}
\begin{itemize}
    \item Actor updates are too small relative to critic's rapid Q-landscape changes
    \item The actor ``chases a moving target'' -- by the time it adapts to the current Q-function, the critic has already shifted
    \item Result: Slow convergence, suboptimal policies
\end{itemize}

\textbf{High Actor LR ($\eta_a \geq 0.01$):}
\begin{itemize}
    \item Large weight updates push pre-activation values into saturation regions
    \item tanh outputs lock at $\pm 1$, gradients vanish
    \item Result: Actor stops learning entirely within a few episodes
\end{itemize}

\textbf{The ``Chasing a Moving Target'' Dynamic:}

The critic learns $10^6\times$ faster than the actor due to gradient magnitude differences. This creates a non-stationary optimization landscape for the actor:

\begin{equation}
\frac{\partial Q}{\partial \theta_c} \gg \frac{\partial \mathcal{L}_a}{\partial \theta_a} \implies \Delta \theta_c \gg \Delta \theta_a
\end{equation}

The actor's policy $\pi_{\theta_a}$ is optimized against $Q_{\theta_c}$, but $\theta_c$ changes substantially between actor updates. The actor effectively optimizes against a stale Q-function, leading to:
\begin{enumerate}
    \item Oscillating policies that never stabilize
    \item Suboptimal convergence to local minima
    \item Increased sensitivity to learning rate selection
\end{enumerate}

This explains why the recommended actor/critic LR ratio of $\sim$0.1 (actor LR $<$ critic LR) works empirically -- it partially compensates for the gradient magnitude imbalance, though it cannot fully resolve the non-stationarity issue.

\subsection{Practical Recommendations}

Based on our analysis, we recommend:

\textbf{Learning Rate Selection:}
\begin{itemize}
    \item Actor learning rate: 0.0001 - 0.001 (conservative)
    \item Critic learning rate: 0.001 - 0.01 (can be 10$\times$ higher than actor)
    \item Recommended ratio: Actor LR / Critic LR $\approx$ 0.1
\end{itemize}

\textbf{Reward Scaling:}
\begin{itemize}
    \item Normalize rewards to unit scale when possible
    \item Monitor effective learning rate = $\eta \times |R|$
    \item Keep effective LR $<$ 0.1 for actors
\end{itemize}

\textbf{Architecture Modifications:}
\begin{itemize}
    \item Consider replacing tanh with scaled sigmoid or softsign
    \item Add gradient clipping on actor updates
    \item Monitor pre-activation magnitudes during training
\end{itemize}

\textbf{Training Monitoring:}
\begin{itemize}
    \item Implement weight change detection for actors
    \item Track tanh output distribution (warning if clustered at $\pm 1$)
    \item Compare actor vs critic gradient magnitudes
\end{itemize}

\subsection{Analysis from Multiple Perspectives}

To ensure comprehensive understanding, we analyze our findings from multiple theoretical perspectives.

\subsubsection{Optimization Perspective}

From an optimization standpoint, the actor and critic are jointly solving a bilevel optimization problem where the actor optimizes policy performance given the critic's value estimates, while the critic learns to accurately predict returns. The gradient asymmetry we observe implies that this joint optimization is inherently unbalanced---the critic's optimization landscape changes faster than the actor can track, violating the quasi-static assumption that underlies many convergence proofs for actor-critic methods~\cite{konda2000actor}.

\subsubsection{Information-Theoretic Perspective}

From an information flow perspective, the critic receives direct reward feedback (high information bandwidth), while the actor receives reward information only indirectly through the critic's Q-value gradients (low information bandwidth). The tanh saturation further reduces this bandwidth by compressing the gradient signal. This information bottleneck explains why actors are more sensitive to hyperparameter choices---they operate with less margin for error in extracting learning signal from their limited information channel.

\subsubsection{Stability Analysis Perspective}

The differential convergence behavior can be viewed through the lens of dynamical systems stability. The actor-critic system has two coupled dynamical subsystems with different time constants. When the actor's effective time constant (inverse learning rate) is too small relative to the critic's, the system enters an unstable regime where the actor overshoots, saturates, and loses the ability to track the critic's changes. Conservative actor learning rates increase the actor's time constant, improving stability at the cost of slower adaptation.

\subsubsection{Robustness of CCM-MADRL Despite Actor Freezing}

An important observation is that CCM-MADRL continues to improve performance even after actors stop updating. This robustness stems from the master agent's selection mechanism: even with fixed actor policies, the critic continues refining Q-value estimates, enabling better selection of which agents to include in the offloading coalition. This architectural feature provides partial immunity to actor stagnation but does not eliminate the underlying gradient pathology.

\subsection{Broader Impact}

This analysis has implications beyond MEC task offloading:

\begin{enumerate}
    \item \textbf{DDPG/TD3/SAC implementations:} All use tanh-bounded outputs and may exhibit similar behavior. TD3's delayed policy updates~\cite{fujimoto2018addressing} may partially mitigate this by reducing actor update frequency, implicitly increasing the effective actor time constant.

    \item \textbf{Hyperparameter transfer:} Learning rates optimal in one domain may fail in others with different reward scales. Our analysis suggests that practitioners should normalize rewards or adjust learning rates proportionally when transferring hyperparameters across domains.

    \item \textbf{Multi-agent systems:} Shared rewards amplify the effective reward scale ($N$ agents $\times$ per-agent contribution), making MARL systems particularly susceptible to the saturation effects we characterize.

    \item \textbf{Reward shaping:} Dense reward shaping, often used to accelerate learning, can inadvertently increase reward magnitude and trigger premature actor saturation if learning rates are not correspondingly reduced.
\end{enumerate}

\subsection{Comparison with Existing Mitigation Techniques}

Several techniques in the literature implicitly address aspects of the gradient asymmetry problem:

\begin{itemize}
    \item \textbf{Delayed policy updates (TD3):} By updating the actor less frequently than the critic, TD3 effectively reduces the actor's learning rate relative to the critic, aligning with our recommendation for conservative actor learning rates.

    \item \textbf{Entropy regularization (SAC):} The entropy bonus in SAC encourages policy diversity, potentially preventing early convergence to saturated outputs. However, this does not directly address the gradient magnitude asymmetry.

    \item \textbf{Target networks:} Target networks stabilize critic learning but do not affect the actor's susceptibility to saturation.

    \item \textbf{Gradient clipping:} While gradient clipping can prevent exploding gradients in critics, it does not address the vanishing gradient problem in actors caused by tanh saturation.
\end{itemize}

Our analysis suggests that these techniques are incomplete solutions. A more direct approach would be to address the output activation asymmetry itself, either through alternative bounded activations or through explicit gradient balancing mechanisms.

\subsection{Summary of Evidence}

Our comprehensive experiments validate the theoretical analysis through multiple lines of evidence:

\begin{enumerate}
    \item \textbf{Gradient asymmetry:} Measured ratio $\rho \sim 10^{-8}$ to $10^{-4}$ confirms 4--8 orders of magnitude critic gradient dominance across all configurations.

    \item \textbf{Saturation correlation:} Strong correlation ($r > 0.9$) between final saturation ratio and stopping episode demonstrates the causal chain from high LR to saturation to weight update cessation.

    \item \textbf{Layer-wise analysis:} Gradient breakdown occurs specifically at the actor output layer (tanh), while hidden layers (ReLU) maintain gradient flow.

    \item \textbf{Consistency:} The pattern holds across all 16 learning rate configurations, with actor LR as the dominant factor and critic LR having secondary effect only at extreme values (0.1).
\end{enumerate}

These findings provide both theoretical explanation and empirical validation for the gradient asymmetry phenomenon in actor-critic architectures.
