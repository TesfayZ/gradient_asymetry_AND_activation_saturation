\section{Background and Problem Formulation}
\label{sec:background}

\subsection{Multi-Agent Markov Decision Process}

We formalize the multi-agent task offloading problem as a Multi-Agent Markov Decision Process (MA-MDP) defined by the tuple $\langle \mathcal{N}, \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$ where:
\begin{itemize}
    \item $\mathcal{N} = \{1, 2, ..., N\}$ is the set of $N$ agents (user devices)
    \item $\mathcal{S} = \mathcal{S}_1 \times \mathcal{S}_2 \times ... \times \mathcal{S}_N$ is the joint state space
    \item $\mathcal{A} = \mathcal{A}_1 \times \mathcal{A}_2 \times ... \times \mathcal{A}_N$ is the joint action space
    \item $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ is the state transition probability
    \item $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the shared reward function
    \item $\gamma \in [0,1)$ is the discount factor
\end{itemize}

\subsection{State and Action Spaces}

\textbf{Per-Agent State} ($s_i \in \mathbb{R}^7$): Each agent's state comprises transmission power $p_i$, channel gain $h_i$, available energy $e_i$, task size $d_i$, required CPU cycles $c_i$, task deadline $\tau_i$, and device compute capability $f_i$.

\textbf{Per-Agent Action} ($a_i \in \mathbb{R}^3$): Each agent outputs an offload decision $x_i \in [-1, 1]$ (offload if $x_i \geq 0$), compute allocation $\alpha_i \in [0, 1]$, and power allocation $\rho_i \in [0, 1]$.

\subsection{Client-Master Architecture}

The CCM-MADRL architecture comprises:

\textbf{Client Agents (Actors):} 50 independent actor networks $\pi_{\theta_i}: \mathcal{S}_i \rightarrow \mathcal{A}_i$ with architecture: $7 \rightarrow 64 \rightarrow 32 \rightarrow 3$ using ReLU hidden activations and \textbf{tanh output}. Each client outputs continuous actions based on local state.

\textbf{Master Agent (Critic):} Single centralized critic network $Q_\phi: \mathcal{S} \times \mathcal{A} \times \mathcal{S}_i \times \mathcal{A}_i \rightarrow \mathbb{R}$ with architecture: $(N \cdot 7 + N \cdot 3 + 7 + 3) \rightarrow 512 \rightarrow 128 \rightarrow 1$ using ReLU hidden and \textbf{linear output}. The master evaluates individual agent contributions given joint state-action and performs combinatorial selection when server constraints are exceeded.

\subsection{Training Dynamics}

\textbf{Critic Update:}
\begin{equation}
\phi \leftarrow \phi - \eta_c \nabla_\phi \frac{1}{B} \sum_{j=1}^B \left( y_j - Q_\phi(s_j, a_j, s_{i,j}, a_{i,j}) \right)^2
\end{equation}
where $y_j = r_j + \gamma \max_{a'} Q_{\phi'}(s'_j, a'_j, s'_{i,j}, a'_{i,j})$.

\textbf{Actor Update:}
\begin{equation}
\theta_i \leftarrow \theta_i + \eta_a \nabla_{\theta_i} \frac{1}{B} \sum_{j=1}^B Q_\phi(s_j, a_j, s_{i,j}, \pi_{\theta_i}(s_{i,j}))
\end{equation}

\subsection{Problem Definition}

\textbf{Phenomenon:} Given identical initialization and exploration sequences, client agents stop updating weights at episode $E_\text{stop}$ where:
\begin{itemize}
    \item $E_\text{stop} \approx 5$ for $\eta_a \in \{0.01, 0.1\}$
    \item $E_\text{stop} > 2000$ for $\eta_a = 0.0001$
    \item The master agent never stops ($E_\text{stop}^{\text{master}} = \infty$ for all $\eta_c$)
\end{itemize}

\textbf{Research Questions:}
\begin{enumerate}
    \item What causes actors to stop updating while critics continue?
    \item Why is the stopping episode learning-rate-dependent?
    \item What architectural factors create this asymmetry?
    \item How can premature actor stopping be prevented?
\end{enumerate}
